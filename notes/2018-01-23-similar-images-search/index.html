<!DOCTYPE html>
<html lang="">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Поиск похожего изображения</title>
  <meta name="description" content="" />
  <!-- Yandex.Metrika counter -->
<meta name="yandex-verification" content="721d8d42ba7b40bb" />
<script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();
    for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}
    k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");
 
    ym(91297055, "init", {
         clickmap:true,
         trackLinks:true,
         accurateTrackBounce:true
    });
 </script>
 <noscript><div><img src="https://mc.yandex.ru/watch/91297055" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
 <!-- /Yandex.Metrika counter -->

  <link rel="stylesheet" href="/styles/index.css">
  <link rel="stylesheet" href="/styles/katex.min.css">
</head>

<body>
  <header>
    <div class="logo">
      <a href="/">
        <!-- Awoo face generated using https://505e06b2.github.io/ -->
        <pre class="ascii-art">
⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢁⣿⡿⢡⣿⣿⣿⣿⢣⣿⣿⣿⣿⣿⢏⣼⣷⣦⡙⢃⣿⣿⣿⢸⣿⣿⣿⣿⣿⣿⣿⣿⡟⠟⣸⣿⡿⣡⣾⣿⡇⢸⣿⣿⡟⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣡⣾⣿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⣾⣿⡇⣿⣿⣿⣿⡇⣿⣿⣿⣿⣿⣡⣿⣿⣿⣿⠄⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠃⣨⣭⢉⣬⣤⣤⣬⡅⢸⣿⣿⢱⣿⣿⣿⣿⣿⣿⣿⣿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⣡
⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢰⣿⡟⠄⣿⣿⣿⣿⣿⣿⣿⣿⢿⢫⣿⣿⣿⣿⣿⣼⣿⣿⣿⣿⡇⣿⣿⣿⣿⣿⣿⠟⣡⢡⡿⢣⣿⣿⣿⣿⣿⡇⢸⣿⡟⣼⣿⣿⣿⣿⣿⣿⣿⣿⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢫⣾⣿
⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⣾⣿⢁⣏⢻⣿⣿⣿⢸⣿⣿⣧⢀⣀⣁⡀⠉⠙⠻⢿⣿⣿⣿⣿⣧⢹⣿⣿⣿⣿⠏⣼⢇⠟⣴⣿⣿⣿⣿⣿⣿⣧⢸⣿⢰⣿⣿⣿⣿⣿⣿⣿⣿⡟⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢋⣴⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⣿⠁⣿⡏⣼⡿⣿⣿⣿⣿⢸⣿⣿⢃⣾⣿⣿⣿⠿⠖⠒⠄⣈⣿⣿⣿⣿⣆⢻⣿⣿⠏⣾⣟⣼⠞⠛⡋⠉⠉⠉⠉⠉⠉⠘⠃⠻⣿⣿⣿⣿⣿⣿⣿⣿⢱⣿⣿⣿⣿⣿⣿⣿⣿⢋⣽⣶⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⣿⢸⣿⢡⣿⡇⣿⣿⣿⡿⢸⡿⢇⣾⣟⣋⣉⣤⣤⣶⣾⣿⣿⣿⣿⣿⣿⣿⣎⡿⣋⣾⣿⣿⣿⣦⣄⡈⠙⠿⠿⣿⣿⣿⠈⣾⣿⣿⣿⣿⣿⣿⣿⣿⢇⣿⣿⣿⣿⣿⣿⠿⢟⣡⣿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⣿⢸⡇⣼⣿⡇⢿⣿⣿⠇⢸⡗⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⠄⠈⠹⡏⣄⣿⣿⣿⣿⣿⣿⣿⣿⣇⣾⣿⣿⣿⡿⠟⣥⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⡿⠸⠡⣿⣿⣟⠸⣿⣿⢸⢸⠷⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⡄⣠⣽⣿⣿⣿⣿⣿⣿⣿⡿⣹⣿⣿⣿⣿⠇⢉⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⣿⠸⢰⣿⣿⣿⡆⢿⣿⢸⣸⣧⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢹⣿⣿⣿⣿⣿⣿⣿⣿⡿⣱⡿⠟⣋⣥⠖⣱⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿
</pre>
      </a>
    </div>
    <table>
      <tbody>
        <tr>
          <td style="width: 33%;"><a href="/teaching">Курсы</a></td>
          <td style="width: 33%;"><a href="/notes">Заметки</a></td>
          <td style="width: 33%;"><a href="/talks">Выступления</a></td>
        </tr>
      </tbody>
    </table>
  </header>

  <div>
    <main>
<article>
  <div>
    <h1>Поиск похожего изображения</h1>
    <time datetime="2018-01-23">23 Jan 2018</time>
  </div>

  <div id="notes-entry-container">
    <content>
      <p>Небольшой отчет о создании приложения KawaiiSearch — поиска похожих фотографий с помощью сверточной нейросети и kNN</p>
<blockquote>
<p>tl;dr; Качаем фотографии из интернетов. Натренированной нейросетью VGG19 вычисляем 4096-мерные вектора каждого изображения. Косинусной метрикой находим ближайшего соседа к целевой картинке. Получаем наиболее похожие картинки в каком-то смысле. Profit!<br>
<a href="https://github.com/senior-sigan/KawaiiSearch">Source code</a>.</p>
</blockquote>
<h2>Введение</h2>
<p>Еще каких-то 5 лет назад, чтобы сделать свой поиск по картинкам, приходилось погружаться в тонкости машинного зрения. Нужно было придумывать то, каким образом преобразовать картинку в некоторый индекс, отпечаток, по которому можно найти другую, похожую на неё. Судя по всему, для этих целей могли использовать перцептивные хеши и вариации, разные преобразования характеристик изображений и даже каскады Хаара. В общем, всё это напоминало классические алгоритмы эпохи до машинного обучения, когда исследователям основываясь на их восприятии самим приходилось придумывать некоторые модели. Это всё безумно интересно и серьезно, можно защитить не один диплом и диссертацию. Но что примечательно, сейчас существует достаточно простой способ для построения своего собственного движка поиска похожих картинок и начать решать свои бизнес задачи немного проще, чем это было раньше.</p>
<h2>Что такое “похожие” изображения</h2>
<p><img src="/assets/similar-images-search/1_jeCYjspklUIiXJb6lYPCTQ.jpg" alt="Sheepdog or mop?"></p>
<p>Прежде чем мы рассмотрим существующие подходы, необходимо правильно поставить вопрос — определить метрику похожести изображений. Но проблема подстерегает нас сразу, так как эта метрика может отличаться от задачи к задаче. Например, нам нужно находить исходное изображение по черно белому экземпляру. А может, необходимо находить картинки близкие по цветовой гамме. Возможна и постановка задачи, когда похожими считаются изображения со схожими формами объектов. А будет ли похожи фотографии одной и той же собаки в разных ракурсах или похожи фотографии разных собак, но в одном ракурсе.</p>
<p>Как видите, есть некоторые трудности в постановке задачи. Обычно даже не говорят, что две фотографии похожи, а рассматриваю некоторую величину похожести от, скажем, нуля — совершенно похожи, до бесконечности — совершенно не похожи. Измерение этой величины будет зависеть от той формы индексов, которые будет давать некоторый алгоритм; это может быть расстояние Хемминга или расстояние между точками в многомерном пространстве, или ещё что-то. Выбор метрики естественно будет влиять на результат не меньше, чем сам алгоритм поиска признаков в изображениях.</p>
<h2>Обзор существующих классических решений</h2>
<p>Если посмотреть на обычные алгоритмические методы сравнения изображений, то в основном всё сводится к вычислению некоторой хеш функции над изображением, а потом вычислению расстояния, например, Хэмминга между двумя значениями хешей. Чем меньше расстояние, тем больше картинки похожи между собой.</p>
<p>Далее начинается типичный для алгоритмических моделей путь выбора некоторой магической функции, которая будет сохранять в себе похожесть изображений. Один из примеров таких функций — это <a href="https://habrahabr.ru/post/120562/">перцептивный хеш</a>. В этом подходе с помощью <a href="https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%81%D0%BA%D1%80%D0%B5%D1%82%D0%BD%D0%BE%D0%B5_%D0%BA%D0%BE%D1%81%D0%B8%D0%BD%D1%83%D1%81%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%B5%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5">дискретного косинусного преобразования</a> оставляют так называемые нижние частоты, в которых сконцентрировано больше информации о форме изображения, чем о его цветовых характеристиках. В итоге большое изображение превращается в 64-битный хеш.</p>
<p>Есть еще несколько алгоритмов, даже на основе <a href="https://habrahabr.ru/post/198338/">каскадов Хаара</a>, но в результате так или иначе, алгоритм очень сильно страдает от преобразований над изображением: от поворотов, отражений, изменений размера, модификации цветности. Но тем не менее они очень быстро работают. Их можно использовать для поиска дубликатов с незначительными искажениями. Но для поиска изображениях, в которых “похожие” определяются в том смысле, что на них изображены коты, а не собаки, алгоритмы этого типа не подходят, да в принципе этого от них и не требуют. Забавно, что еще в 2011 году в комментариях к статьям писали, что невозможно написать такой алгоритм для кошек-собак.</p>
<p>Предлагаемый подход весьма наивный и простой, основывается на использовании машинного обучения и нейросетей. Так сразу в лоб и не совсем понятно чему надо обучать модель. Мы сами не можем сформулировать понятие “похожесть”. Тем не менее есть способ, и для его использования нам понадобится для начала решить задачу классификации.</p>
<h2>Классификация изображений</h2>
<p>На качественно ином уровне задачу классификации изображений начали решать с 2013 года. Тогда на наборе данных ImageNet пробили барьер в 15% ошибок классификации тысячи видов объектов. С тех пор за 5 лет было спроектировано и натренированно очень много разных моделей нейросетей, и был пробит барьер в 5% ошибок. Самыми успешными из них считаются: <a href="https://arxiv.org/abs/1409.1556">VGG16</a>, <a href="https://arxiv.org/abs/1512.03385">ResNet50</a>, <a href="https://arxiv.org/abs/1512.00567">Inception</a>, <a href="https://arxiv.org/abs/1409.4842">GoogLeNet</a> и много других. Большинство их них построено на основе <a href="http://cs231n.github.io/convolutional-networks/">свёрточных нейросетей</a>.</p>
<p>На рисунке вы можете посмотреть как выглядит схематически архитектура <a href="https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/">VGG16</a>.</p>
<p><img src="/assets/similar-images-search/1_dMRA5zbKqLX4rRul_X2gkA.png" alt="VGG16"></p>
<p>Слои нейросети на изображении состоят из набора разных фильтров-сверток. Каждый из фильтров отвечает за поиск определенного шаблона, и когда он находит некоторый участок изображения, в котором есть этот узор, то фильтр посылает сигнал в следующий слой. В свою очередь сигналы предыдущего слоя составляют новое изображение для следующего слоя. На рисунке архитектуры VGG16 вы можете видеть, что сначала было цветное RGB изображение размера 224x224 пикселей с 3 каналами(red, green, blue). Потом после прохода первого слоя сверток у нас получилось изображение размера 224x224 пикселей с 64 каналами. Эти каналы уже представляют не цвета, а результаты работы каждого из 64 фильтров-свёрток. И так далее, до изображения 7x7 пикселей с 512 каналами.</p>
<p><img src="/assets/similar-images-search/1_aIJ-Dm4mxEwC-Jpf7oxf4g.png" alt="Свёртка ищет бублик"></p>
<iframe src="https://www.youtube.com/embed/p*7GWRup-nQ*" width="640" height="360" frameborder="0" allow="picture-in-picture" allowfullscreen>Свёртка ищет бублик</iframe>
<p>Строя каскады свёрточных слоёв и обучая модель, вы получаете слои, содержащие в себе абстракции изображений. Первые слои в себе могут содержать мелкие детали: линии. Далее идут комбинации деталей — фигуры. Следующие слои уже могут содержать формы, а в конце целые объекты.</p>
<p><img src="/assets/similar-images-search/1_v4l_IajKqOm53Z7U34bkKQ.jpg" alt=" ">
<em>Feature Visualization of Convnet trained on ImageNet from [Zeiler &#x26; Fergus 2013]</em></p>
<p>Обратите внимание еще на одну интересную особенность свёрточных слоев в этой модели: каждый следующий слой “толще”, так как в нём больше фильтров, но “меньше”, так как изображение специально уменьшают с помощью операции MaxPooling (субдискретизация). Используют этот прием по следующей причине: важнее факт детекции некоторого признака-объекта, чем знание точного расположения этого объекта на изображении. Именно поэтому берут максимум внутри небольшого окошка, тем самым создавая карту расположений признаков.</p>
<p><img src="/assets/similar-images-search/1_hrWnZ25CNMAEs-34BThqsA.png" alt="">
<em>Max Pool 2x2, <a href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></em></p>
<p>Ближе к выходу модели у нас имеется маленькое изображение — карта признаков размера 7x7 пикселей с 512 фильтрами. По этой трёхмерной карте всё еще невозможно сделать предсказания классов объектов на изображении — котик или собака. Для того чтобы перейти уже к предсказаниям классов, эту карту укладывают на плоскости с помощью операции Flatten и соединяют с полносвязным скрытым слоем из 4096 нейронов. А дальше классическая схема: еще один скрытый слой с 4096 нейронами и выходной слой с 1000 нейронами, каждый из которых выдает вероятность принадлежности к одному из 1000 классов в задаче ImageNet.</p>
<h2>Fine Tuning и переиспользование модели</h2>
<p>Как оказалось, сети обученные на данных ImageNet можно переиспользовать для других задач компьютерного зрения. Например, у нас задача отличать кошку от собаки. Вам не надо создавать свою модель CatDogVGG, искать миллионы картинок и обучать с нуля сеть. Всё куда проще! Вы берёте <em>предобученную модель</em> VGG, срезаете последние полносвязные слои нейронов, отвечающие за финальную классификацию(да, так можно), оставляя только внутренние 4096 нейронов, которые соединяете со своими 2 нейрона для выхода кошка-собака. Получается, что вам нужно будет только дообучить модель этим 2*4096 связям, что делается легко и быстро.</p>
<p>Какой физический смысл в срезании последнего слоя сети и подстановки нового? Оказывается, что все слои свертки внутри себя заключают способность “понимать” изображение. А поскольку обучение происходило на тысяче разных классов, то и обобщающая способность этих слоев достаточно сильная. В итоге внешние 4096 нейронов на самом деле выдают вектор характеристик(признаков) любого изображения в целом. Поэтому для того чтобы проходила наша классификация, отличная от изначальной, нам остается дообучить нейросеть только и только переводить этот 4096-мерный вектор в наш вектор предсказаний принадлежности классов, не меняя существующую глубокую свёрточную сеть.</p>
<p>Подобный финт можно провернуть не только с VGG, но и с другими свёрточными архитектурами распознавания изображений. Для этой цели в библиотеке Keras есть даже специальные конструкты, которые вы можете изучить это в разделе <a href="https://keras.io/applications/">keras-applications</a>. Распознавание образов еще никогда не было таким простым!</p>
<h2>Поиск похожих изображений с помощью нейросети</h2>
<p>Как мы поняли из предыдущего параграфа, срезанная нейросеть производит по своей сути извлечение признаков из изображения и переводит изображение в осмысленный вектор. Получение такого вектора раньше требовало экспертной оценки и придумывания очередного алгоритма хеширований. Полученные теми методами вектора (64-битные хешсуммы) заключали в себе информацию, в лучшем случае, о контурах и простых формах в целом. Нейросеть же даёт как минимум 4096-мерное представление, в котором заключена и форма, и цвет, и целые объекты.</p>
<p>Хорошо, нами получен вектор признаков изображения, что мы делаем дальше? Считать расстояние Хэмминга, как мы это делали с хешами, тут бессмысленно. Здесь мы должны использовать другой подход. Представьте, каждый из векторов куда-то направлен в пространстве, это направление характеризует изображение. Если мы посчитаем вектора для множества картинок, то логично предположить, что похожие картинки будут иметь вектора характеристик расположенные в пространстве близко. Отличной метрикой близости векторов в многомерном пространстве служит <a href="https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C#%D0%9A%D0%BE%D1%81%D0%B8%D0%BD%D1%83%D1%81%D0%BD%D0%BE%D0%B5_%D1%81%D1%85%D0%BE%D0%B4%D1%81%D1%82%D0%B2%D0%BE">косинусная метрика</a>, хорошо себя зарекомендовавшая в задачах классификации текстов.</p>
<p><img src="/assets/similar-images-search/1_2dwMgH7oScs3cxjy4Scb4Q.png" alt="Косинусная метрика">
<em>Косинусная метрика</em></p>
<p>Чем меньше метрика, тем ближе объекты в векторном пространстве, тем больше похожи изображения по “мнению” нейросети.</p>
<h2>Собираем модель как конструктор</h2>
<p>Отлично, мы знаем теорию, знаем как получить представление нейросети об изображениях, знаем как сравнивать эти представления. Осталось дело за малым — собрать всё воедино.</p>
<p>Я предлагаю построить веб-приложение, похожее на Google Image Search. Для его построения нам понадобятся следующие библиотеки для Python3:</p>
<ul>
<li><a href="https://keras.io/">Keras</a> и <a href="https://www.tensorflow.org/">Tensorflow</a> для работы с нейросетями;</li>
<li>Numpy (a.k.a np) для математических функций;</li>
<li><a href="http://scikit-learn.org/">Sklearn</a> для алгоритма ближайших соседей и косинусной метрики;</li>
<li><a href="http://flask.pocoo.org/">Flask</a> для веба;</li>
<li>Pandas, pillow, scipy, h5py для разных вспомогательных нужд.</li>
</ul>
<p>Ещё нужно откуда-то взять много изображений на одну тематику. Я скачал все фотографии из блога <a href="http://tokyo-fashion.tumblr.com/">tokio-fashion</a> (около 50 тысяч фото), надеясь что нейросеть будет находить похожие образы или позы или еще что-нибудь. Кстати анализ fashion-индустрии это отдельная интересная область исследований!</p>
<p>Опишем базовые use-cases, которые мы хотим реализовать:</p>
<ul>
<li>пользователь заходит на страницу;</li>
<li>пользователь жмёт кнопку “мне повезет”, тем самым выбирая случайную картинку из всего набора данных;</li>
<li>сервер ищет методом ближайших соседей K самых близких вектора к случайно выбранному, эти K векторов будут описывать самые похожие картинки;</li>
<li>пользователь видит на странице исходную картинку и 9 похожих с метрикой похожести в подписи.</li>
</ul>
<p>Как делать веб часть и так все знают, поэтому рассмотрим наиболее неясные шаги.</p>
<h2>Векторизация базы фотографий</h2>
<p>Как мы уже знаем, для векторизации нам нужно отрезать один из последних слоев предобученной нейросети. Это операция распространенная, поэтому в библиотеке Keras нам уже доступны из коробки, во-первых, сама <a href="https://keras.io/applications/#vgg16">модель с весами</a> и, во-вторых, возможность не включать последний слой. Это делается следующим образом:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> keras.applications <span class="hljs-keyword">import</span> VGG19

model = VGG19(weights=<span class="hljs-string">'imagenet'</span>, include_top=<span class="hljs-literal">False</span>)
</code></pre>
<p>Для более тонкой настройки можно указать имя определенного слоя. Как найти имя слоя — это отдельный анекдот, поэтому я смотрел в <a href="https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py#L149">исходники</a> модели.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> keras.applications <span class="hljs-keyword">import</span> VGG19
<span class="hljs-keyword">from</span> keras.engine <span class="hljs-keyword">import</span> Model

bm = VGG19(weights=<span class="hljs-string">'imagenet'</span>)
model = Model(inputs=bm.<span class="hljs-built_in">input</span>, outputs=bm.get_layer(<span class="hljs-string">'fc1'</span>).output)
</code></pre>
<p>После того как у вас загружена модель, она, кстати, будет реально загружать из интернета веса, можно конвертировать все изображения в вектора с помощью метода <code>predict</code>, что логично, так как мы “предсказываем” этот вектор.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> keras.preprocessing <span class="hljs-keyword">import</span> image
<span class="hljs-keyword">from</span> keras.applications.vgg19 <span class="hljs-keyword">import</span> preprocess_input

img = image.load_img(path, target_size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)) <span class="hljs-comment"># чтение из файла</span>
x = image.img_to_array(img)  <span class="hljs-comment"># сырое изображения в вектор</span>
x = np.expand_dims(x, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># превращаем в вектор-строку (2-dims)</span>
x = preprocess_input(x) <span class="hljs-comment">#  библиотечная подготовка изображения</span>
vec = model.predict(x).ravel()
<span class="hljs-comment"># ... PROFIT!</span>
</code></pre>
<p>Теперь всё тайное стало явью, и осталось дело техники — итерироваться по всем изображениям и для каждого из него произвести векторизацию. Потом сохранить эти вектора в БД или csv файла, не важно. Это мы оставим за <a href="https://github.com/blan4/KawaiiSearch/blob/master/src/vectorize_image.py">кулисами</a>.</p>
<h2>Поиск похожего методом kNN</h2>
<p>Далее нам нужно реализовать поиск ближайших векторов к целевому вектору с помощью косинусной метрики. В мире маш.обуча. не принято писать такие низкоуровневые задачи, поэтому мы переиспользуем код для алгоритма обучения без учителя — ближайшие соседи. В библиотеке sklearn есть специальный класс <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors">NearestNeighbors</a>, которому можно указать метрику по которой он будет искать ближайших соседей к целевому объекту. Подготовка этого компонента будет выглядеть следующим образом:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> NearestNeighbors

knn = NearestNeighbors(metric=<span class="hljs-string">'cosine'</span>, algorithm=<span class="hljs-string">'brute'</span>)
vecs = load_images_vectors() <span class="hljs-comment">#  а это мы уже сделали!</span>
knn.fit(vecs)
</code></pre>
<p>Мы выбрали <code>cosine</code> метрику и активировали полный перебор, загрузили все вектора в <code>knn</code> методом <code>fit</code>. Обычно этот метод запускает обучение, но в данном случае он просто сохранит все вектора внутри объекта <code>knn</code>, так как это алгоритм обучения без учителя.</p>
<p>Чтобы получить ближайших соседей, то есть похожие изображения, пишем следующе:</p>
<pre><code class="hljs language-python">filenames = load_images_filenames()
vec = load_target_image()

dist, indices = knn.kneighbors(vec, n_neighbors=<span class="hljs-number">10</span>)
similar_images = [
    (filenames[indices[i]], dist[i])
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(indices))
]
</code></pre>
<p>В <code>similar_images</code> у нас будет лежать массив из 10 пар: имя файла и значение похожести по метрике, которая чем меньше, тем более похожее изображение к целевому. Всё, теперь этот список можно отдавать на фронтенд и рисовать красивую галерею. Изменяя параметр <code>n_neighbours</code> можно менять количество возвращаемых соседей, которые упорядочены по увеличению метрики, то есть дальше будут более непохожие картинки.</p>
<p>Тонкий момент. Функция <code>load_images_filenames()</code> возвращает список файлов ровно в том же порядке, в котором они перечислены в массиве <code>load_images_vectors()</code>, так как нам нужно точное соответствие вектора картинке.</p>
<h2>Результат</h2>
<p>Теперь вы видите, что с точки зрения кода задача теперь стала весьма простой. Но это всё благодаря развитию нейросетей и добрым экспериментаторам, которые выкладывают предобученные модели в интернет, иначе вам пришлось бы это всё самим обучать очень долго и мучительно.</p>
<p>А что в результате получилось? С моим результатом вы можете ознакомиться на сайте <a href="http://sigan.1der.link/kawaii_search">kawaii-search</a>, исходники которого доступны на <a href="https://github.com/blan4/KawaiiSearch">github</a>. Всё это крутиться на сервере <a href="https://cloud.google.com/compute/docs/machine-types">google-cloud n1-standard-1</a> c 3.75GB RAM чего не хватает и пришлось добавить еще столько же swap. Так как задача предсказания не сильно сложная, то видеокарта не нужна.</p>
<p>А в каком смысле теперь определено “похоже”. В случае с fashion датасетом, похожими считаются изображения, на которых есть объекты одних классов. Например, фотографии с только с портфелями, только с обувью одного типа, портреты, прически, руки. Смотрите сами:</p>
<p><img src="/assets/similar-images-search/1_JcWGJYElsBfOre5MWjiYBQ.jpg" alt="Портфели"></p>
<p><img src="/assets/similar-images-search/1_OOVwoR09K3KFi5jESyubag.jpg" alt="Обувь"></p>
<p><img src="/assets/similar-images-search/1_Trtn711gR6-Dj7d7Yh3ukg.jpg" alt="Колье, чокер"></p>
<p>Что здесь интересного? Сеть сама решила что для нее похожее. Мы это не контролировали, оно получилось само. Мы потеряли некоторый контроль, но мы можем его вернуть, если сделаем обучения сами. И меня впечатляет то, что я не знаю как можно всё это сделать обычными алгоритмическими методами.</p>
<p>Кстати, на загруженных пользователем фотографиях поиск тоже работает. Обратите внимание, что здесь похожесть выражается в том, что на всех картинках есть один и тот же предмет — деревянный меч. Датасет в этом примере другой и модель немного хуже, поэтому есть ошибки.</p>
<p><img src="/assets/similar-images-search/1_q13lid2-vXlvogF7ITfBng.jpg" alt=" ">
<em>Похож = есть один и тот же предмет</em></p>
<h2>Что можно с этим делать</h2>
<p>Какие еще реальные задачи можно решать подобным этому подходом? Приведу список первого, что пришло в голову:</p>
<ul>
<li>Написать своё приложение под Android для поиска книги в магазине по фотографии обложки.</li>
<li>Приложение-экскурсовод, которое говорит что за здание на фотографии.</li>
<li>Простая модель идентификации человека по лицу.</li>
<li>Я думаю о том, как бы создать приложение для поиска похожих стилей в fashion-фото.</li>
</ul>
<p>А что еще можно сделать? Предлагайте в комментариях!</p>
<h2>Литература</h2>
<ul>
<li><a href="https://habrahabr.ru/post/120562/">Выглядит похоже. Как работает перцептивный хэш</a></li>
<li><a href="https://habrahabr.ru/post/122372/">Алгоритмы быстрого нахождения похожих изображений</a></li>
<li><a href="https://habrahabr.ru/post/198338/">Использование каскада Хаара для сравнения изображений</a></li>
<li><a href="https://habrahabr.ru/post/237307/">Как бороться с репостами или пара слов о перцептивных хэшах</a></li>
<li><a href="http://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual Recognition</a></li>
<li><a href="https://keras.io/applications/">Applications - Keras Documentation</a></li>
<li><a href="http://cs231n.github.io/transfer-learning/">CS231n transfer learning</a></li>
<li><a href="https://github.com/senior-sigan/KawaiiSearch">Исходные коды Kawaii-Search</a></li>
<li><a href="http://kawaii-search.senior-sigan.net/">Kawaii-Search - Демка поиска похожих</a></li>
</ul>
    </content>
  </div>
</article></main>
  </div>
</body>

</html>